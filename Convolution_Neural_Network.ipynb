{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e416f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "\n",
    "class MatrixMultiplicationLayer:\n",
    "    def __init__(self, X, W):\n",
    "        self.X = X\n",
    "        self.W = W\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dX = (self.W)\n",
    "        self.doutput_dW = (self.X).T \n",
    "\n",
    "class BiasAdditionLayer:\n",
    "    \n",
    "    def __init__(self, output : np.ndarray , B : np.ndarray):\n",
    "        self.B = B\n",
    "        self.output = output\n",
    "    \n",
    "    def forward(self):\n",
    "        self.output = self.output + self.B\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dB = np.identity(self.B.shape[1])\n",
    "\n",
    "class MeanSquaredLossLayer:\n",
    "\n",
    "    def __init__(self, target: np.ndarray, predicted: np.ndarray):\n",
    "\n",
    "        self.predicted = predicted\n",
    "        self.target = target \n",
    "        \n",
    "    def forward(self):\n",
    "        error = self.predicted - self.target\n",
    "        self.L = np.mean(np.square(error))\n",
    "\n",
    "    def backward(self):\n",
    "        self.dL_dpredicted = (2 / len(self.target)) * (self.predicted - self.target).T\n",
    "        \n",
    "# softmax(x)i = e^(x_i)/∑ j={1,n}e^(x_j)\n",
    "# Here,softmax(x)i represents the i-th element of the output vector after applying the softmax function to \n",
    "# x, and x_i represents the i-th element of the input vector x. The softmax function normalizes the values of \n",
    "# x into a probability distribution where each element of the output vector \n",
    "# softmax(x) is in the range [0, 1] and the sum of all elements equals 1.\n",
    "\n",
    "class SoftmaxLayer:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def forward(self):\n",
    "        exp_X = np.exp(self.Z - np.max(self.Z, axis=1, keepdims=True))\n",
    "        self.output = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.diag(self.output.reshape(-1)) - (self.output.T)@((self.output))\n",
    "        \n",
    "# sigmoid(x)= 1/1+e^−x\n",
    "# Here,sigmoid(x) represents the output of the sigmoid function for input x. The sigmoid function maps any \n",
    "# real-valued number to the range [0, 1]. It has an S-shaped curve, with the function outputting values \n",
    "# close to 0 for large negative inputs and values close to 1 for large positive inputs. It is often used \n",
    "# to convert raw scores into probabilities in binary classification tasks or as an activation function \n",
    "# in neural networks\n",
    "\n",
    "class SigmoidLayer:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = 1 / (1 + np.exp(-self.Z))\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.diag(np.multiply(self.output,(1 - self.output)).reshape(-1))\n",
    "        \n",
    "# CrossEntropyLoss= (1/N) * ∑ i={1,N} ∑ c={1,C} (yi,c * log(p i,c)) \n",
    "# Here:\n",
    "\n",
    "# N is the number of samples.\n",
    "\n",
    "# C is the number of classes.\n",
    "\n",
    "# yi,c is a binary indicator (0 or 1) for whether class c is the correct classification for sample i.\n",
    "\n",
    "# pi,c is the predicted probability that sample i belongs to class c.\n",
    "\n",
    "# The Cross-Entropy Loss penalizes models more heavily when they make large errors in classification, \n",
    "# as the logarithm term amplifies the loss for confident, incorrect predictions. It is commonly used as \n",
    "# the loss function in multi-class classification problems, especially when combined with softmax activation \n",
    "# in the output layer of neural networks.\n",
    "\n",
    "class CrossEntropyLossLayer:\n",
    "\n",
    "    def __init__(self, target,predicted):\n",
    "        self.target = target\n",
    "        self.predicted = predicted\n",
    "        \n",
    "    def forward(self):\n",
    "        epsilon = 1e-15\n",
    "        self.L = -np.sum(self.target * np.log(self.predicted + epsilon))\n",
    "    def backward(self):\n",
    "        self.dL_dpredicted =  -1*((self.target) / (len(self.predicted)+1e-40)).T\n",
    "\n",
    "# LinearActivation(x)=x\n",
    "# While Linear Activation is rarely used within hidden layers of neural networks due to its inability \n",
    "# to introduce non-linearity, it is often used as the activation function in the output layer of regression \n",
    "# models, where the network is tasked with predicting continuous values.\n",
    "class LinearActivation:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.Z\n",
    "         \n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.identity(self.Z.shape[1])\n",
    "        \n",
    "# TanhActivation(x)= (e^x - e^−x) /(e^x + e^−x)\n",
    "# The Tanh Activation function is often used in hidden layers of neural networks to introduce non-linearity. \n",
    "# It is particularly useful when dealing with data that has negative values since it can map negative inputs to \n",
    "# negative outputs.\n",
    "\n",
    "class TanhActivation:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = np.tanh(self.Z)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.doutput_dZ =  np.diag(1 - self.output.reshape(-1)**2)\n",
    "# ReLU(x)=max(0,x)\n",
    "# ReLU has become the default choice for many neural network architectures due to its simplicity and \n",
    "# effectiveness in combating the vanishing gradient problem during training. It is particularly effective in \n",
    "# deep neural networks, where it helps mitigate the vanishing gradient problem by allowing gradients to flow \n",
    "# more freely during backpropagation.\n",
    "\n",
    "class ReLUActivation:\n",
    "\n",
    "    def __init__(self, Z): \n",
    "        self.Z = Z \n",
    "        self.Leak = 0.01\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = np.maximum(0,self.Z)\n",
    "      \n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.diag([1. if x>=0 else 0.01 for x in self.output.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67380a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data_sklearn(dataset_name='california', normalize_X=False, normalize_y=False, one_hot_encode_y=False, test_size=0.2):\n",
    "    if dataset_name == 'california':\n",
    "        california_data = fetch_california_housing()\n",
    "        data = {'data': california_data.data, 'target': california_data.target}       \n",
    "    elif dataset_name == 'iris':\n",
    "        iris_data = load_iris()\n",
    "        data = {'data': iris_data.data, 'target': iris_data.target}\n",
    "    elif dataset_name == 'mnist':\n",
    "        mnist_data = load_digits()\n",
    "        data = {'data': (mnist_data.data >= 8), 'target': (mnist_data.target).astype(int)}\n",
    "\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1, 1)\n",
    "\n",
    "    if normalize_X:\n",
    "        normalizer = Normalizer()\n",
    "        X = normalizer.fit_transform(X)\n",
    "\n",
    "    if normalize_y:\n",
    "        normalizer = Normalizer()\n",
    "        y = normalizer.fit_transform(y)\n",
    "\n",
    "    if one_hot_encode_y:\n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1aeee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, dimenshion_of_input,output_neuron, activation_name=\"linear\", seed=42):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.dimenshion_of_input = dimenshion_of_input\n",
    "        self.output_neuron = output_neuron\n",
    "        \n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random((1, self.dimenshion_of_input))   # assigned during SGD\n",
    "        self.Z = np.random.random((1, self.output_neuron))\n",
    "        \n",
    "        self.W = np.random.random((dimenshion_of_input, output_neuron)) * \\\n",
    "            np.sqrt(2 / (dimenshion_of_input + output_neuron))\n",
    "        self.B = np.random.random((1, output_neuron))*np.sqrt(2 / (1 + output_neuron))\n",
    "        \n",
    "        self.multiply_layer = MatrixMultiplicationLayer(self.X, self.W)\n",
    "        self.bias_layer = BiasAdditionLayer(self.B, self.B)\n",
    "\n",
    "        if activation_name == 'linear':\n",
    "            self.activation_layer = LinearActivation(self.Z)\n",
    "        elif activation_name == 'sigmoid':\n",
    "            self.activation_layer = SigmoidLayer(self.Z)\n",
    "        elif activation_name == 'softmax':\n",
    "            self.activation_layer = SoftmaxLayer(self.Z)\n",
    "        elif activation_name == 'tanh':\n",
    "            self.activation_layer = TanhActivation(self.Z)\n",
    "        elif activation_name == 'relu':\n",
    "            self.activation_layer = ReLUActivation(self.Z)\n",
    "            \n",
    "    def forward_layer(self):\n",
    "        self.multiply_layer.forward()\n",
    "        self.bias_layer.output = self.multiply_layer.output\n",
    "        self.bias_layer.forward()\n",
    "        self.activation_layer.Z = self.bias_layer.output\n",
    "        self.activation_layer.forward()\n",
    "        self.Z = self.activation_layer.output\n",
    "    def backward_layer(self):\n",
    "        self.activation_layer.backward()\n",
    "        self.bias_layer.backward()\n",
    "        self.multiply_layer.backward()\n",
    "\n",
    "class NeuralNetwork(Layer):\n",
    "    \"\"\"\n",
    "    Input  - layers : list of layer objects , loss_name : Name of loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    # [ \"mean_squared\", \"cross_entropy\"]\n",
    "    def __init__(self, layers, loss_name=\"mean_squared\", learning_rate=0.01, seed=42):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)  # number of layers in neural network\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.inp_shape = self.layers[0].X.shape\n",
    "        self.out_shape = self.layers[-1].Z.shape\n",
    "\n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random(self.inp_shape)   # assigned during SGD\n",
    "        self.Y = np.random.random(self.out_shape)  # output of neural network\n",
    "\n",
    "        # define loss layer\n",
    "        if loss_name == \"mean_squared\":\n",
    "            self.loss_layer = MeanSquaredLossLayer(self.Y, self.Y)\n",
    "        if loss_name == \"cross_entropy\":\n",
    "            self.loss_layer = CrossEntropyLossLayer(self.Y, self.Y)\n",
    "\n",
    "    def forward(self):\n",
    "        self.layers[0].X = self.X\n",
    "        self.loss_layer.target = self.Y\n",
    "\n",
    "        self.layers[0].forward_layer()\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.layers[i].X = self.layers[i-1].Z\n",
    "            self.layers[i].forward_layer()\n",
    "\n",
    "        self.loss_layer.predicted = self.layers[-1].Z\n",
    "        self.loss_layer.forward()\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "#         self.loss_layer.Z = self.Y\n",
    "        self.loss_layer.backward()\n",
    "        self.grad_nn = self.loss_layer.dL_dpredicted\n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            self.layers[i].backward_layer()\n",
    "\n",
    "            dL_dZ = np.dot(\n",
    "                self.layers[i].activation_layer.doutput_dZ, self.grad_nn)\n",
    "            dL_dW = np.dot(self.layers[i].multiply_layer.doutput_dW, dL_dZ.T)\n",
    "            dL_dB = np.dot(self.layers[i].bias_layer.doutput_dB, dL_dZ).T\n",
    "\n",
    "            # Update W & B\n",
    "            self.layers[i].W -= self.learning_rate*dL_dW\n",
    "            self.layers[i].B -= self.learning_rate*dL_dB\n",
    "\n",
    "            # Update outer_grad\n",
    "            self.grad_nn = np.dot(self.layers[i].multiply_layer.doutput_dX, dL_dZ)\n",
    "\n",
    "            del dL_dZ, dL_dW, dL_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72080cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLayers(inp_shape, layers_sizes, layers_activations):\n",
    "    layers = []\n",
    "    n_layers = len(layers_sizes)\n",
    "    layer_0 = Layer(inp_shape, layers_sizes[0], layers_activations[0])\n",
    "    layers.append(layer_0)\n",
    "    inp_shape_next = layers_sizes[0]\n",
    "    for i in range(1, n_layers):\n",
    "        layer_i = Layer(inp_shape_next, layers_sizes[i], layers_activations[i])\n",
    "        layers.append(layer_i)\n",
    "        inp_shape_next = layers_sizes[i]\n",
    "\n",
    "    out_shape = inp_shape_next\n",
    "    return inp_shape, out_shape, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46605f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CNN Model\n",
    "\n",
    "def rotateMatrix(mat):\n",
    "    N = len(mat)\n",
    "    rot_mat = np.zeros((N, N))\n",
    "\n",
    "    for t1 in range(N):\n",
    "        k = N - 1\n",
    "        for t2 in range(N):\n",
    "            rot_mat[t1][t2] = mat[k][N - 1 - t2]\n",
    "        k -= 1\n",
    "\n",
    "    return rot_mat\n",
    "\n",
    "# flatten operation:\n",
    "\n",
    "def flatten(inp_mat):\n",
    "    flatten_vector = []\n",
    "\n",
    "    for i in range(len(inp_mat)):  # number of rows\n",
    "        for j in range(len(inp_mat[0])):  # number of columns\n",
    "            flatten_vector.append(inp_mat[i][j])\n",
    "\n",
    "    flatten_vector = np.array(flatten_vector)\n",
    "    return flatten_vector\n",
    "\n",
    "class ConvolutionalLayer:\n",
    "    \n",
    "    def __init__(self, input_shape, activation='tanh', filter_shape=(1, 1), learning_rate=0.01, num_output_channels=1, seed=42):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        assert (input_shape[1] >= filter_shape[0] and input_shape[2] >= filter_shape[1]), \\\n",
    "            f\"Error: Input shape {input_shape} is incompatible with filter shape {filter_shape}\"\n",
    "\n",
    "        self.input_shape = input_shape\n",
    "        self.num_input_channels = input_shape[0]  \n",
    "        self.num_output_channels = num_output_channels  \n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Initialize filters, biases, output, and flatten output\n",
    "        self.filter_shape = (self.num_output_channels, self.num_input_channels, *filter_shape)\n",
    "        self.filters = np.random.rand(*self.filter_shape)\n",
    "        self.biases = np.random.rand(self.num_output_channels, input_shape[1] - filter_shape[0] + 1, input_shape[2] - filter_shape[1] + 1)\n",
    "        self.output_shape = self.biases.shape\n",
    "        self.flatten_shape = np.prod(self.output_shape)\n",
    "        self.output = np.zeros(self.output_shape)\n",
    "        self.flatten_output = np.zeros((1, self.flatten_shape))\n",
    "\n",
    "        # Define activation function\n",
    "        if activation == 'tanh':\n",
    "            self.activation_layer = TanhActivation(self.output)\n",
    "\n",
    "    def forward(self):\n",
    "        # Add bias to output\n",
    "        self.output = self.biases.copy()\n",
    "\n",
    "        # Perform convolution\n",
    "        for i in range(self.num_output_channels):\n",
    "            for j in range(self.num_input_channels):\n",
    "                self.output[i] += self.convolve(self.input[j], self.filters[i, j])\n",
    "\n",
    "        # Flatten output\n",
    "        self.flatten_output = self.output.reshape(1, -1)\n",
    "\n",
    "        # Forward pass through activation layer\n",
    "        self.activation_layer.Z = self.flatten_output\n",
    "        self.activation_layer.forward()\n",
    "\n",
    "    def backward(self, gradient_nn):\n",
    "        # Backward pass through activation layer\n",
    "        self.activation_layer.backward()\n",
    "        loss_gradient = np.dot(self.activation_layer.doutput_dZ, gradient_nn)\n",
    "\n",
    "        # Reshape loss gradient to match output shape\n",
    "        loss_gradient = loss_gradient.reshape(self.output_shape)\n",
    "\n",
    "        # Initialize gradients\n",
    "        self.filters_gradient = np.zeros(self.filter_shape)\n",
    "        self.input_gradient = np.zeros(self.input_shape)\n",
    "        self.biases_gradient = loss_gradient\n",
    "\n",
    "        # Pad loss gradient\n",
    "        padded_loss_gradient = np.pad(loss_gradient, (\n",
    "            (0, 0),\n",
    "            (self.filter_shape[2] - 1, self.filter_shape[2] - 1),\n",
    "            (self.filter_shape[3] - 1, self.filter_shape[3] - 1)\n",
    "        ))\n",
    "\n",
    "        # Compute gradients for filters and input\n",
    "        i = 0\n",
    "        while i < self.num_output_channels:\n",
    "            j = 0\n",
    "            while j < self.num_input_channels:\n",
    "                # Compute filter gradient\n",
    "                self.filters_gradient[i, j] = self.convolve(self.input[j], loss_gradient[i])\n",
    "                \n",
    "                # Rotate the filter by 180 degrees\n",
    "                rot180_filter_ij = np.rot90(np.rot90(self.filters[i, j], axes=(0, 1)), axes=(0, 1))\n",
    "                \n",
    "                # Compute input gradient\n",
    "                k = 0\n",
    "                while k < self.num_output_channels:\n",
    "                    l = 0\n",
    "                    while l < self.num_input_channels:\n",
    "                        self.input_gradient[l] += self.convolve(padded_loss_gradient[k], rot180_filter_ij)\n",
    "                        l += 1\n",
    "                    k += 1\n",
    "                    l = 0  # Reset inner loop index\n",
    "                \n",
    "                j += 1\n",
    "            i += 1\n",
    "        \n",
    "        # Update filters and biases\n",
    "        self.filters -= self.learning_rate * self.filters_gradient\n",
    "        self.biases -= self.learning_rate * self.biases_gradient\n",
    "\n",
    "    def convolve(self, x, y):\n",
    "        x_conv_y = np.zeros((x.shape[0] - y.shape[0] + 1, x.shape[1] - y.shape[1] + 1))\n",
    "        i = 0\n",
    "        while i < x.shape[0] - y.shape[0] + 1:\n",
    "            j = 0\n",
    "            while j < x.shape[1] - y.shape[1] + 1:\n",
    "                tmp = x[i:i + y.shape[0], j:j + y.shape[1]]\n",
    "                tmp = np.multiply(tmp, y)\n",
    "                x_conv_y[i, j] = np.sum(tmp)\n",
    "                j += 1\n",
    "            i += 1\n",
    "        return x_conv_y\n",
    "\n",
    "\n",
    "class CNN : \n",
    "    \"\"\"\n",
    "    Implementation of Convolutional Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                convolutional_layer,                   # convolutional layer \n",
    "                nn,                                    # feed forward neural network\n",
    "                seed = 42): \n",
    "\n",
    "        self.nn = nn \n",
    "        self.convolutional_layer = convolutional_layer \n",
    "        self.X = _ # assigned during SGD \n",
    "        self.Y = _ # assigned during SGD \n",
    "    \n",
    "    def forward(self):\n",
    "        # forward pass of convolutional layer \n",
    "        self.convolutional_layer.input = self.X \n",
    "        self.convolutional_layer.forward()\n",
    "\n",
    "        # forward pass of neural network \n",
    "        self.nn.X = self.convolutional_layer.activation_layer.output\n",
    "        self.nn.Y = self.Y \n",
    "        self.nn.forward()  \n",
    "    \n",
    "    def backward(self): \n",
    "        # backward pass of neural network \n",
    "        self.nn.backward() \n",
    "\n",
    "        # backward pass of convolutional network \n",
    "        self.convolutional_layer.backward( self.nn.grad_nn ) \n",
    "\n",
    "def SGD_CNN(X_train,\n",
    "            y_train,\n",
    "            X_test,\n",
    "            y_test,\n",
    "            cnn,\n",
    "            inp_shape,\n",
    "            out_shape,\n",
    "            n_iterations=1000,\n",
    "            task=\"classification\"):\n",
    "    \n",
    "    # Define a custom progress bar function\n",
    "    def progress_bar(iteration, total_iterations):\n",
    "        progress = (iteration + 1) / total_iterations\n",
    "        bar_length = 30\n",
    "        filled_length = int(bar_length * progress)\n",
    "        bar = '=' * filled_length + '-' * (bar_length - filled_length)\n",
    "        print(f'\\rTraining ... [{bar}] {progress * 100:.2f}%', end='', flush=True)\n",
    "        if iteration == total_iterations - 1:\n",
    "            print()\n",
    "            \n",
    "    # Initialize iteration counter\n",
    "    iteration = 0\n",
    "\n",
    "    # Loop until iteration reaches n_iterations\n",
    "    while iteration < n_iterations:\n",
    "        # Display the progress bar\n",
    "        progress_bar(iteration, n_iterations)\n",
    "        randomIndx = np.random.randint(len(X_train))\n",
    "        X_sample = X_train[randomIndx, :].reshape(inp_shape)\n",
    "        Y_sample = y_train[randomIndx, :].reshape(out_shape)\n",
    "\n",
    "        cnn.X = X_sample\n",
    "        cnn.Y = Y_sample\n",
    "\n",
    "        cnn.forward()  # Forward Pass\n",
    "        cnn.backward()  # Backward Pass\n",
    "\n",
    "        # Increment the iteration counter\n",
    "        iteration += 1\n",
    "\n",
    "    # Clear the progress bar after training\n",
    "    print('\\033[K', end='', flush=True)\n",
    "    \n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error\n",
    "\n",
    "    if task == \"classification\":\n",
    "        X_train = X_train.reshape(-1, 8, 8)\n",
    "        y_true = np.argmax(y_train, axis=1)\n",
    "        acc = 0\n",
    "        for i in range(len(X_train)):\n",
    "            cnn.X = X_train[i][np.newaxis, :, :]\n",
    "            cnn.Y = y_train[i]\n",
    "            cnn.forward()\n",
    "            y_pred_i = np.argmax(cnn.nn.loss_layer.predicted, axis=1)\n",
    "            if (y_pred_i == y_true[i]):\n",
    "                acc += 1\n",
    "        \n",
    "        print(\"Classification Accuracy (Training Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )\n",
    "\n",
    "        X_test = X_test.reshape(-1, 8, 8)\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        acc = 0\n",
    "        for i in range(len(X_test)):\n",
    "            cnn.X = X_test[i][np.newaxis, :, :]\n",
    "            cnn.Y = y_test[i]\n",
    "            cnn.forward()\n",
    "            y_pred_i = np.argmax(cnn.nn.loss_layer.predicted, axis=1)\n",
    "            if (y_pred_i == y_true[i]):\n",
    "                acc += 1\n",
    "        \n",
    "        print(\"Classification Accuracy (Testing Data ):\" + str(acc) + \"/\" + str(len(y_true)) + \" = \" + str(acc*100/len(y_true)) + \" %\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e79282",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data_sklearn('mnist', one_hot_encode_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e03c6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... [==============================] 100.00%\n",
      "\u001b[KClassification Accuracy (Training Data ):149/1437 = 10.368823938761308 %\n",
      "Classification Accuracy (Testing Data ):29/360 = 8.055555555555555 %\n"
     ]
    }
   ],
   "source": [
    "conv_inp_shape = (1,8,8)   # sklearn digit dataset has images of shape 1 x 8 x 8\n",
    "Co = 16  # 16 channel output \n",
    "conv_filter_shape = (3,3)\n",
    "conv_activation = 'tanh'\n",
    "convolutional_layer = ConvolutionalLayer(conv_inp_shape,\n",
    "                                        activation = conv_activation,\n",
    "                                        filter_shape = conv_filter_shape, \n",
    "                                        learning_rate = 0.01,\n",
    "                                        num_output_channels = Co\n",
    "                                        )\n",
    "nn_inp_shape = convolutional_layer.flatten_shape \n",
    "layers_sizes = [10]\n",
    "layers_activations = ['softmax']\n",
    "\n",
    "nn_inp_shape, nn_out_shape, layers = createLayers(nn_inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'cross_entropy'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "cnn = CNN(convolutional_layer, nn)\n",
    "out_shape =  (1, layers_sizes[-1])  # one_hot encoded ouptut \n",
    "\n",
    "SGD_CNN(X_train,y_train,X_test,y_test, cnn,conv_inp_shape, out_shape,n_iterations=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
