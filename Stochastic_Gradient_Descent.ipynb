{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6cf18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import Normalizer, OneHotEncoder\n",
    "\n",
    "class MatrixMultiplicationLayer:\n",
    "    def __init__(self, X, W):\n",
    "        self.X = X\n",
    "        self.W = W\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = np.dot(self.X, self.W)\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dX = (self.W)\n",
    "        self.doutput_dW = (self.X).T \n",
    "\n",
    "class BiasAdditionLayer:\n",
    "    \n",
    "    def __init__(self, output : np.ndarray , B : np.ndarray):\n",
    "        self.B = B\n",
    "        self.output = output\n",
    "    \n",
    "    def forward(self):\n",
    "        self.output = self.output + self.B\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dB = np.identity(self.B.shape[1])\n",
    "\n",
    "class MeanSquaredLossLayer:\n",
    "\n",
    "    def __init__(self, target: np.ndarray, predicted: np.ndarray):\n",
    "\n",
    "        self.predicted = predicted\n",
    "        self.target = target \n",
    "        \n",
    "    def forward(self):\n",
    "        error = self.predicted - self.target\n",
    "        self.L = np.mean(np.square(error))\n",
    "\n",
    "    def backward(self):\n",
    "        self.dL_dpredicted = (2 / len(self.target)) * (self.predicted - self.target).T\n",
    "        \n",
    "# softmax(x)i = e^(x_i)/∑ j={1,n}e^(x_j)\n",
    "# Here,softmax(x)i represents the i-th element of the output vector after applying the softmax function to \n",
    "# x, and x_i represents the i-th element of the input vector x. The softmax function normalizes the values of \n",
    "# x into a probability distribution where each element of the output vector \n",
    "# softmax(x) is in the range [0, 1] and the sum of all elements equals 1.\n",
    "\n",
    "class SoftmaxLayer:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def forward(self):\n",
    "        exp_X = np.exp(self.Z - np.max(self.Z, axis=1, keepdims=True))\n",
    "        self.output = exp_X / np.sum(exp_X, axis=1, keepdims=True)\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.diag(self.output.reshape(-1)) - (self.output.T)@((self.output))\n",
    "        \n",
    "# sigmoid(x)= 1/1+e^−x\n",
    "# Here,sigmoid(x) represents the output of the sigmoid function for input x. The sigmoid function maps any \n",
    "# real-valued number to the range [0, 1]. It has an S-shaped curve, with the function outputting values \n",
    "# close to 0 for large negative inputs and values close to 1 for large positive inputs. It is often used \n",
    "# to convert raw scores into probabilities in binary classification tasks or as an activation function \n",
    "# in neural networks\n",
    "\n",
    "class SigmoidLayer:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = 1 / (1 + np.exp(-self.Z))\n",
    "\n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.diag(np.multiply(self.output,(1 - self.output)).reshape(-1))\n",
    "        \n",
    "# CrossEntropyLoss= (1/N) * ∑ i={1,N} ∑ c={1,C} (yi,c * log(p i,c)) \n",
    "# Here:\n",
    "\n",
    "# N is the number of samples.\n",
    "\n",
    "# C is the number of classes.\n",
    "\n",
    "# yi,c is a binary indicator (0 or 1) for whether class c is the correct classification for sample i.\n",
    "\n",
    "# pi,c is the predicted probability that sample i belongs to class c.\n",
    "\n",
    "# The Cross-Entropy Loss penalizes models more heavily when they make large errors in classification, \n",
    "# as the logarithm term amplifies the loss for confident, incorrect predictions. It is commonly used as \n",
    "# the loss function in multi-class classification problems, especially when combined with softmax activation \n",
    "# in the output layer of neural networks.\n",
    "\n",
    "class CrossEntropyLossLayer:\n",
    "\n",
    "    def __init__(self, target,predicted):\n",
    "        self.target = target\n",
    "        self.predicted = predicted\n",
    "        \n",
    "    def forward(self):\n",
    "        epsilon = 1e-15\n",
    "        self.L = -np.sum(self.target * np.log(self.predicted + epsilon))\n",
    "    def backward(self):\n",
    "        self.dL_dpredicted =  -1*((self.target) / (len(self.predicted)+1e-40)).T\n",
    "\n",
    "# LinearActivation(x)=x\n",
    "# While Linear Activation is rarely used within hidden layers of neural networks due to its inability \n",
    "# to introduce non-linearity, it is often used as the activation function in the output layer of regression \n",
    "# models, where the network is tasked with predicting continuous values.\n",
    "class LinearActivation:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z \n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = self.Z\n",
    "         \n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.identity(self.Z.shape[1])\n",
    "        \n",
    "# TanhActivation(x)= (e^x - e^−x) /(e^x + e^−x)\n",
    "# The Tanh Activation function is often used in hidden layers of neural networks to introduce non-linearity. \n",
    "# It is particularly useful when dealing with data that has negative values since it can map negative inputs to \n",
    "# negative outputs.\n",
    "\n",
    "class TanhActivation:\n",
    "\n",
    "    def __init__(self, Z):\n",
    "        self.Z = Z\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = np.tanh(self.Z)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.doutput_dZ =  np.diag(1 - self.output.reshape(-1)**2)\n",
    "# ReLU(x)=max(0,x)\n",
    "# ReLU has become the default choice for many neural network architectures due to its simplicity and \n",
    "# effectiveness in combating the vanishing gradient problem during training. It is particularly effective in \n",
    "# deep neural networks, where it helps mitigate the vanishing gradient problem by allowing gradients to flow \n",
    "# more freely during backpropagation.\n",
    "\n",
    "class ReLUActivation:\n",
    "\n",
    "    def __init__(self, Z): \n",
    "        self.Z = Z \n",
    "        self.Leak = 0.01\n",
    "        \n",
    "    def forward(self):\n",
    "        self.output = np.maximum(0,self.Z)\n",
    "      \n",
    "    def backward(self):\n",
    "        self.doutput_dZ = np.diag([1. if x>=0 else 0.01 for x in self.output.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d363c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data_sklearn(dataset_name='california', normalize_X=False, normalize_y=False, one_hot_encode_y=False, test_size=0.2):\n",
    "    if dataset_name == 'california':\n",
    "        california_data = fetch_california_housing()\n",
    "        data = {'data': california_data.data, 'target': california_data.target}       \n",
    "    elif dataset_name == 'iris':\n",
    "        iris_data = load_iris()\n",
    "        data = {'data': iris_data.data, 'target': iris_data.target}\n",
    "    elif dataset_name == 'mnist':\n",
    "        mnist_data = load_digits()\n",
    "        data = {'data': (mnist_data.data >= 8), 'target': (mnist_data.target).astype(int)}\n",
    "\n",
    "    X = data['data']\n",
    "    y = data['target'].reshape(-1, 1)\n",
    "\n",
    "    if normalize_X:\n",
    "        normalizer = Normalizer()\n",
    "        X = normalizer.fit_transform(X)\n",
    "\n",
    "    if normalize_y:\n",
    "        normalizer = Normalizer()\n",
    "        y = normalizer.fit_transform(y)\n",
    "\n",
    "    if one_hot_encode_y:\n",
    "        encoder = OneHotEncoder()\n",
    "        y = encoder.fit_transform(y).toarray()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1422736",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, dimenshion_of_input,output_neuron, activation_name=\"linear\", seed=42):\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        self.dimenshion_of_input = dimenshion_of_input\n",
    "        self.output_neuron = output_neuron\n",
    "        \n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random((1, self.dimenshion_of_input))   # assigned during SGD\n",
    "        self.Z = np.random.random((1, self.output_neuron))\n",
    "        \n",
    "        self.W = np.random.random((dimenshion_of_input, output_neuron)) * \\\n",
    "            np.sqrt(2 / (dimenshion_of_input + output_neuron))\n",
    "        self.B = np.random.random((1, output_neuron))*np.sqrt(2 / (1 + output_neuron))\n",
    "        \n",
    "        self.multiply_layer = MatrixMultiplicationLayer(self.X, self.W)\n",
    "        self.bias_layer = BiasAdditionLayer(self.B, self.B)\n",
    "\n",
    "        if activation_name == 'linear':\n",
    "            self.activation_layer = LinearActivation(self.Z)\n",
    "        elif activation_name == 'sigmoid':\n",
    "            self.activation_layer = SigmoidLayer(self.Z)\n",
    "        elif activation_name == 'softmax':\n",
    "            self.activation_layer = SoftmaxLayer(self.Z)\n",
    "        elif activation_name == 'tanh':\n",
    "            self.activation_layer = TanhActivation(self.Z)\n",
    "        elif activation_name == 'relu':\n",
    "            self.activation_layer = ReLUActivation(self.Z)\n",
    "            \n",
    "    def forward_layer(self):\n",
    "        self.multiply_layer.forward()\n",
    "        self.bias_layer.output = self.multiply_layer.output\n",
    "        self.bias_layer.forward()\n",
    "        self.activation_layer.Z = self.bias_layer.output\n",
    "        self.activation_layer.forward()\n",
    "        self.Z = self.activation_layer.output\n",
    "    def backward_layer(self):\n",
    "        self.activation_layer.backward()\n",
    "        self.bias_layer.backward()\n",
    "        self.multiply_layer.backward()\n",
    "\n",
    "class NeuralNetwork(Layer):\n",
    "    \"\"\"\n",
    "    Input  - layers : list of layer objects , loss_name : Name of loss layer\n",
    "    \"\"\"\n",
    "\n",
    "    # [ \"mean_squared\", \"cross_entropy\"]\n",
    "    def __init__(self, layers, loss_name=\"mean_squared\", learning_rate=0.01, seed=42):\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        self.layers = layers\n",
    "        self.n_layers = len(layers)  # number of layers in neural network\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.inp_shape = self.layers[0].X.shape\n",
    "        self.out_shape = self.layers[-1].Z.shape\n",
    "\n",
    "        # random initialization of input X  and output Z\n",
    "        self.X = np.random.random(self.inp_shape)   # assigned during SGD\n",
    "        self.Y = np.random.random(self.out_shape)  # output of neural network\n",
    "\n",
    "        # define loss layer\n",
    "        if loss_name == \"mean_squared\":\n",
    "            self.loss_layer = MeanSquaredLossLayer(self.Y, self.Y)\n",
    "        if loss_name == \"cross_entropy\":\n",
    "            self.loss_layer = CrossEntropyLossLayer(self.Y, self.Y)\n",
    "\n",
    "    def forward(self):\n",
    "        self.layers[0].X = self.X\n",
    "        self.loss_layer.target = self.Y\n",
    "\n",
    "        self.layers[0].forward_layer()\n",
    "        for i in range(1, self.n_layers):\n",
    "            self.layers[i].X = self.layers[i-1].Z\n",
    "            self.layers[i].forward_layer()\n",
    "\n",
    "        self.loss_layer.predicted = self.layers[-1].Z\n",
    "        self.loss_layer.forward()\n",
    "\n",
    "    def backward(self):\n",
    "\n",
    "#         self.loss_layer.Z = self.Y\n",
    "        self.loss_layer.backward()\n",
    "        self.grad_nn = self.loss_layer.dL_dpredicted\n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            self.layers[i].backward_layer()\n",
    "\n",
    "            dL_dZ = np.dot(\n",
    "                self.layers[i].activation_layer.doutput_dZ, self.grad_nn)\n",
    "            dL_dW = np.dot(self.layers[i].multiply_layer.doutput_dW, dL_dZ.T)\n",
    "            dL_dB = np.dot(self.layers[i].bias_layer.doutput_dB, dL_dZ).T\n",
    "\n",
    "            # Update W & B\n",
    "            self.layers[i].W -= self.learning_rate*dL_dW\n",
    "            self.layers[i].B -= self.learning_rate*dL_dB\n",
    "\n",
    "            # Update outer_grad\n",
    "            self.grad_nn = np.dot(self.layers[i].multiply_layer.doutput_dX, dL_dZ)\n",
    "\n",
    "            del dL_dZ, dL_dW, dL_dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e09ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLayers(inp_shape, layers_sizes, layers_activations):\n",
    "    layers = []\n",
    "    n_layers = len(layers_sizes)\n",
    "    layer_0 = Layer(inp_shape, layers_sizes[0], layers_activations[0])\n",
    "    layers.append(layer_0)\n",
    "    inp_shape_next = layers_sizes[0]\n",
    "    for i in range(1, n_layers):\n",
    "        layer_i = Layer(inp_shape_next, layers_sizes[i], layers_activations[i])\n",
    "        layers.append(layer_i)\n",
    "        inp_shape_next = layers_sizes[i]\n",
    "\n",
    "    out_shape = inp_shape_next\n",
    "    return inp_shape, out_shape, layers\n",
    "\n",
    "def SGD_NeuralNetwork(X_train,\n",
    "                      y_train,\n",
    "                      X_test,\n",
    "                      y_test,\n",
    "                      nn,\n",
    "                      inp_shape=1,   # dimension of input\n",
    "                      out_shape=1,   # dimension of output\n",
    "                      n_iterations=1000,\n",
    "                      task=\"regression\"  # [ \"regression\", \"classification\"]\n",
    "                      ):\n",
    "    # Define a custom progress bar function\n",
    "    def progress_bar(iteration, total_iterations):\n",
    "        progress = (iteration + 1) / total_iterations\n",
    "        bar_length = 30\n",
    "        filled_length = int(bar_length * progress)\n",
    "        bar = '=' * filled_length + '-' * (bar_length - filled_length)\n",
    "        print(f'\\rTraining ... [{bar}] {progress * 100:.2f}%', end='', flush=True)\n",
    "        if iteration == total_iterations - 1:\n",
    "            print()\n",
    "\n",
    "    # Initialize iteration counter\n",
    "    iteration = 0\n",
    "\n",
    "    # Loop until iteration reaches n_iterations\n",
    "    while iteration < n_iterations:\n",
    "        # Display the progress bar\n",
    "        progress_bar(iteration, n_iterations)\n",
    "\n",
    "        random_index = np.random.randint(len(X_train))\n",
    "        X_sample = X_train[random_index, :].reshape(1, inp_shape)\n",
    "        Y_sample = y_train[random_index, :].reshape(1, out_shape)\n",
    "\n",
    "        nn.X = X_sample\n",
    "        nn.Y = Y_sample\n",
    "\n",
    "        nn.forward()  # Forward Pass\n",
    "        nn.backward()  # Backward Pass\n",
    "\n",
    "        # Increment the iteration counter\n",
    "        iteration += 1\n",
    "\n",
    "    # Clear the progress bar after training\n",
    "    print('\\033[K', end='', flush=True)\n",
    "\n",
    "    # Lets run ONLY forward pass for train and test data and check accuracy/error\n",
    "\n",
    "    if task == \"regression\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        train_error = nn.loss_layer.L\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "\n",
    "        nn.forward()\n",
    "\n",
    "        test_error = nn.loss_layer.L\n",
    "\n",
    "        if isinstance(nn.loss_layer, MeanSquaredLossLayer):\n",
    "            print(\"Mean Squared Loss Error (Train Data)  : %0.5f\" % train_error)\n",
    "            print(\"Mean Squared Loss Error (Test Data)  : %0.5f\" % test_error)\n",
    "\n",
    "    if task == \"classification\":\n",
    "        nn.X = X_train\n",
    "        nn.Y = y_train\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_train, axis=1)\n",
    "        y_pred = np.argmax(nn.loss_layer.predicted, axis=1)\n",
    "        acc = 1 * (y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Training Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc) * 100 / len(acc)))\n",
    "\n",
    "        nn.X = X_test\n",
    "        nn.Y = y_test\n",
    "        nn.forward()\n",
    "        y_true = np.argmax(y_test, axis=1)\n",
    "        y_pred = np.argmax(nn.loss_layer.predicted, axis=1)\n",
    "        acc = 1 * (y_true == y_pred)\n",
    "        print(\"Classification Accuracy (Testing Data ): {0}/{1} = {2} %\".format(\n",
    "            sum(acc), len(acc), sum(acc) * 100 / len(acc)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7baedab",
   "metadata": {},
   "source": [
    "# CALIFORNIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16005c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data_sklearn('california', normalize_X=True, normalize_y=False, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c7b72d",
   "metadata": {},
   "source": [
    "# One output neural with Linear activation and least mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57325604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... [==============================] 100.00%\n",
      "\u001b[KMean Squared Loss Error (Train Data)  : 2.03453\n",
      "Mean Squared Loss Error (Test Data)  : 2.05931\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [1]\n",
    "layers_activations = ['linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.1)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=12111,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec0daa",
   "metadata": {},
   "source": [
    "# 2 layers. Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with one output neuron and linear activation. use mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f587709e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... [==============================] 100.00%\n",
      "\u001b[KMean Squared Loss Error (Train Data)  : 1.38606\n",
      "Mean Squared Loss Error (Test Data)  : 1.42777\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,1]\n",
    "layers_activations = ['sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.01)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e83fe9",
   "metadata": {},
   "source": [
    "# Three layers. Layer 1 with 13 output neurons with sigmoid activation. Layer 2 with 13 output neurons and sigmoid activation. Layer 3 with one output neuron and linear activation. use mean squared loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac8f1c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ... [==============================] 100.00%\n",
      "\u001b[KMean Squared Loss Error (Train Data)  : 1.32945\n",
      "Mean Squared Loss Error (Test Data)  : 1.36843\n"
     ]
    }
   ],
   "source": [
    "inp_shape = X_train.shape[1]\n",
    "layers_sizes = [13,13,1]\n",
    "layers_activations = ['sigmoid','sigmoid','linear']\n",
    "\n",
    "inp_shape, out_shape, layers = createLayers(inp_shape, layers_sizes, layers_activations)\n",
    "loss_nn = 'mean_squared'\n",
    "\n",
    "nn = NeuralNetwork(layers, loss_nn, learning_rate=0.001)\n",
    "\n",
    "SGD_NeuralNetwork(X_train,y_train,X_test,y_test,nn,inp_shape, out_shape,n_iterations=1000,task=\"regression\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
